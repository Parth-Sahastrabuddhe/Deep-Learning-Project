{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\sahas\\appdata\\local\\temp\\pip-req-build-krn6ztjx\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from clip==1.0) (24.1)\n",
      "Requirement already satisfied: regex in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from clip==1.0) (4.66.5)\n",
      "Requirement already satisfied: torch in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from clip==1.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from clip==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sahas\\anaconda3\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369570 sha256=7828b1d0e31496d666ebd4f5956d417d95a0825ac10fb5e20864f65dd82074c3\n",
      "  Stored in directory: C:\\Users\\sahas\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ytoguyip\\wheels\\35\\3e\\df\\3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: ftfy, clip\n",
      "Successfully installed clip-1.0 ftfy-6.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\sahas\\AppData\\Local\\Temp\\pip-req-build-krn6ztjx'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import clip  # OpenAI CLIP library\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_image = 'Dataset/coco_images/train2017'\n",
    "coco_annotation = 'Dataset/coco_annotations/captions_train2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transform=None):\n",
    "        import json, os\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the COCO caption annotations\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.annotations = data['annotations']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann = self.annotations[idx]\n",
    "        caption = ann['caption']\n",
    "        image_id = ann['image_id']\n",
    "        \n",
    "        # Construct the filename from the image_id (COCO images typically named as 12-digit ID)\n",
    "        filename = f\"{image_id:012d}.jpg\"\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        if not os.path.isfile(image_path):\n",
    "            # If the file is missing, raise an exception or skip\n",
    "            raise FileNotFoundError(f\"Missing file: {image_path}\")\n",
    "        \n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Low resolution for demonstration\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def collate_fn_filter_none(batch):\n",
    "    # Filter out any None values from the batch\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    # Return an empty batch if all samples are None (handle as needed)\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return default_collate(batch)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = COCODataset(image_dir=coco_image, annotation_file=coco_annotation, transform=transform)\n",
    "\n",
    "# After creating your dataset object:\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Use only 10k images for a quicker experiment\n",
    "subset_indices = indices[:10000]\n",
    "\n",
    "small_dataset = Subset(dataset, subset_indices)\n",
    "\n",
    "dataloader = DataLoader(small_dataset, batch_size=64, shuffle=True, num_workers=4, collate_fn=collate_fn_filter_none)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:55<00:00, 6.39MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model_clip, preprocess_clip = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, text_dim=512, feature_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim + text_dim, feature_dim * 8 * 4 * 4),\n",
    "            nn.BatchNorm1d(feature_dim * 8 * 4 * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(feature_dim * 8, feature_dim * 4, 4, 2, 1, bias=False),  # 4x4 -> 8x8\n",
    "            nn.BatchNorm2d(feature_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, 4, 2, 1, bias=False),  # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(feature_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_dim * 2, feature_dim, 4, 2, 1, bias=False),        # 16x16 -> 32x32\n",
    "            nn.BatchNorm2d(feature_dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_dim, 3, 4, 2, 1, bias=False),                      # 32x32 -> 64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text_embed):\n",
    "        x = torch.cat([noise, text_embed], dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, text_dim=512, feature_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, feature_dim, 4, 2, 1, bias=False),  # 64x64 -> 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_dim, feature_dim * 2, 4, 2, 1, bias=False),  # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(feature_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_dim * 2, feature_dim * 4, 4, 2, 1, bias=False),  # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(feature_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_dim * 4, feature_dim * 8, 4, 2, 1, bias=False),  # 8x8 -> 4x4\n",
    "            nn.BatchNorm2d(feature_dim * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.fc = nn.Linear(feature_dim * 8 * 4 * 4 + text_dim, 1)\n",
    "\n",
    "    def forward(self, img, text_embed):\n",
    "        x = self.conv(img)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.cat([x, text_embed], dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "text_dim = 512\n",
    "G = Generator(noise_dim=noise_dim, text_dim=text_dim).to(device)\n",
    "D = Discriminator(text_dim=text_dim).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizerG = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions) in enumerate(dataloader):\n",
    "        batch_size = images.size(0)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Encode captions using CLIP\n",
    "        text_tokens = clip.tokenize(captions).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_embeddings = model_clip.encode_text(text_tokens)  # shape: (batch, 512)\n",
    "        \n",
    "        # Labels for real and fake images\n",
    "        real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "        \n",
    "        # ------------- Update Discriminator -------------\n",
    "        D.zero_grad()\n",
    "        # Real images loss\n",
    "        outputs_real = D(images, text_embeddings)\n",
    "        d_loss_real = criterion(outputs_real, real_labels)\n",
    "        d_loss_real.backward()\n",
    "        \n",
    "        # Generate fake images\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        fake_images = G(noise, text_embeddings)\n",
    "        outputs_fake = D(fake_images.detach(), text_embeddings)\n",
    "        d_loss_fake = criterion(outputs_fake, fake_labels)\n",
    "        d_loss_fake.backward()\n",
    "        \n",
    "        optimizerD.step()\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        # ------------- Update Generator -------------\n",
    "        G.zero_grad()\n",
    "        outputs_fake_for_G = D(fake_images, text_embeddings)\n",
    "        g_loss = criterion(outputs_fake_for_G, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(dataloader)}], \"\n",
    "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Save a grid of generated images and model checkpoints after each epoch\n",
    "    fake_images_sample = fake_images[:16].detach().cpu()\n",
    "    grid = utils.make_grid(fake_images_sample, nrow=4, normalize=True)\n",
    "    utils.save_image(grid, f\"generated_epoch_{epoch+1}.png\")\n",
    "    torch.save(G.state_dict(), f\"generator_epoch_{epoch+1}.pth\")\n",
    "    torch.save(D.state_dict(), f\"discriminator_epoch_{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
